{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "tfd = tfp.distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news_data(categories=None, verbose=True, max_df=0.2):\n",
    "    # Load the 20 newsgroups dataset\n",
    "    newsgroup_data_train = fetch_20newsgroups(subset='train', categories=categories)\n",
    "    newsgroup_data_test = fetch_20newsgroups(subset='test', categories=categories)\n",
    "    orig_Data = [newsgroup_data_train, newsgroup_data_test]\n",
    "    # Vectorize the data using the count vectorizer\n",
    "    n_docs = len(newsgroup_data_train['data'])\n",
    "    # preprocess the data\n",
    "    vectorizer = CountVectorizer(\n",
    "        input='content',\n",
    "        analyzer='word',\n",
    "        stop_words='english',\n",
    "        binary=True, \n",
    "        max_df=0.2,  # 0.2*11314=2262.8\n",
    "        min_df=1.01/n_docs,\n",
    "    )\n",
    "    X_train = vectorizer.fit_transform(newsgroup_data_train.data).todense()\n",
    "    X_test = vectorizer.transform(newsgroup_data_test.data).todense()\n",
    "    y_train = newsgroup_data_train.target\n",
    "    y_test = newsgroup_data_test.target\n",
    "    Data = [X_train, X_test, y_train, y_test]\n",
    "    class_names = newsgroup_data_train.target_names\n",
    "    idx2word = {v: k for k, v in vectorizer.vocabulary_.items()}\n",
    "    if verbose:\n",
    "        print('Train data shape:', newsgroup_data_train.filenames.shape)\n",
    "        print('Test data shape:', newsgroup_data_test.filenames.shape)\n",
    "        print('Vocabulary size:', len(vectorizer.vocabulary_))\n",
    "        print('Number of classes:', np.max(y_train) + 1)\n",
    "    \n",
    "    return Data, orig_Data, idx2word, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (2320,)\n",
      "Test data shape: (1544,)\n",
      "Vocabulary size: 18619\n",
      "Number of classes: 4\n"
     ]
    }
   ],
   "source": [
    "categs = ['sci.space', 'rec.sport.baseball', 'comp.graphics', 'talk.politics.guns']\n",
    "\n",
    "\n",
    "data, orig_data, idx2word, class_names = get_news_data(categories=categs)\n",
    "X_train, X_test, y_train, y_test = data\n",
    "newsgroup_data_train, newsgroup_data_test = orig_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BernoulliNaiveBayes():\n",
    "\n",
    "    def __init__(self, alpha=1.0):\n",
    "        \"\"\"\n",
    "        alpha: float, Laplace smoothing parameter.\n",
    "        \"\"\"\n",
    "        self.alpha = alpha\n",
    "        self.class_priors = None\n",
    "        self.class_conditionals = None\n",
    "        self.n_classes = None\n",
    "        self.n_features = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.n_classes = len(np.unique(y))\n",
    "        self.n_features = X.shape[1]\n",
    "        self.class_priors = self.compute_class_prioirs(y)\n",
    "        self.class_conditionals = self.compute_class_conditionals(X, y)\n",
    "\n",
    "    def compute_class_prioirs(self, y):\n",
    "        N = len(y)\n",
    "        class_priors = np.zeros(self.n_classes)\n",
    "        for i in range(self.n_classes):\n",
    "            class_priors[i] = np.sum(y == i) / N\n",
    "        assert np.sum(class_priors) == 1, 'Class priors should sum to 1'\n",
    "        return tf.cast(class_priors, tf.float32)\n",
    "    \n",
    "    def compute_class_conditionals(self, X, y):\n",
    "        class_conditionals = np.zeros((self.n_classes, self.n_features))\n",
    "        for i in range(self.n_classes):\n",
    "            X_class = X[y == i]\n",
    "            n_class = X_class.shape[0]\n",
    "            class_conditionals[i] = (np.sum(X_class, axis=0) + self.alpha) / (n_class + self.alpha * 2)\n",
    "        return tf.cast(class_conditionals, tf.float32)\n",
    "    \n",
    "    def make_distribution(self):\n",
    "        bernoulli_batch = tfd.Bernoulli(probs=self.class_conditionals)\n",
    "        bernoulli_ind = tfd.Independent(bernoulli_batch, reinterpreted_batch_ndims=1)\n",
    "        return bernoulli_ind\n",
    "    \n",
    "    def predict_logprob_single(self, X):\n",
    "        bernoulli_ind = self.make_distribution()\n",
    "        cond_log_probs = bernoulli_ind.log_prob(X)\n",
    "        joint_log_probs = tf.add(tf.math.log(self.class_priors), cond_log_probs)\n",
    "        p_x = tf.reduce_logsumexp(joint_log_probs, axis=-1, keepdims=True)\n",
    "        log_prob = joint_log_probs - p_x\n",
    "        return log_prob\n",
    "    \n",
    "    def predict(self, X, prob=False):\n",
    "        log_probs = np.zeros((X.shape[0], self.n_classes))\n",
    "        for i in range(X.shape[0]):\n",
    "            log_probs[i] = self.predict_logprob_single(X[i])\n",
    "        if prob:\n",
    "            return tf.exp(log_probs)\n",
    "        else:\n",
    "            return log_probs\n",
    "        \n",
    "    def predict_class(self, X):\n",
    "        return np.argmax(self.predict(X), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb = BernoulliNaiveBayes(alpha=1.0)\n",
    "bnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=float32, numpy=array([  0.      , -27.061478, -50.856857, -71.56946 ], dtype=float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example prediction\n",
    "bnb.predict_logprob_single(X_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8944\n",
      "F1 Score: 0.8970\n"
     ]
    }
   ],
   "source": [
    "# Test Performance\n",
    "test_probs = bnb.predict(X_test, prob=True)\n",
    "test_preds = bnb.predict_class(X_test)\n",
    "\n",
    "acc = accuracy_score(y_test, test_preds)\n",
    "f1 = f1_score(y_test, test_preds, average='macro')\n",
    "\n",
    "print(f'Accuracy: {acc:.4f}')\n",
    "print(f'F1 Score: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True label: comp.graphics\n",
      "Predicted label: comp.graphics\n",
      "\n",
      "Content:\n",
      " From: carlos@carlos.jpr.com (Carlos Dominguez)\n",
      "Subject: Re: Where did the hacker ethic go?\n",
      "Reply-To: carlos@carlos.jpr.com\n",
      "Organization: Private Helldiver/Usenet system, Brooklyn, NY, USA\n",
      "Lines: 38\n",
      "X-Newsreader: Helldiver 1.07 (Waffle 1.65)\n",
      "\n",
      "In <1sp4qj$243@dorsai.dorsai.org> crawls@dorsai.dorsai.org (Charles Rawls) writes:\n",
      "\n",
      ">The hacker ethic is ALIVE and WELL here.  I know of what you speak, and my\n",
      ">only answer is \"SCREW 'EM\".  You have to do what make you feel right.\n",
      "\n",
      "amen.. I too have learned by example, specifically yours. :)\n",
      "\n",
      ">What can I say but keep the faith, there are others who do likewise.\n",
      "\n",
      ".. but dorsai leads the way.. Unlike other services that are commercial\n",
      "in nature, dorsai is a community based service. While others charge\n",
      "monthly fees for access, dorsai accepts donations from those who can\n",
      "afford to contribute.\n",
      "\n",
      "   While other systems don't respond to user input, dorsai thrives on it.\n",
      "Other systems sell hardware for a profit, dorsai donates hardware to\n",
      "community service \n",
      "==================================================\n",
      "True label: rec.sport.baseball\n",
      "Predicted label: rec.sport.baseball\n",
      "\n",
      "Content:\n",
      " From: tedward@cs.cornell.edu (Edward [Ted] Fischer)\n",
      "Subject: Re: Bases loaded walk gives Reds win in 12\n",
      "Organization: Cornell Univ. CS Dept, Ithaca NY 14853\n",
      "Lines: 87\n",
      "\n",
      "In article <mssC5y5u0.4Dn@netcom.com> mss@netcom.com (Mark Singer) writes:\n",
      ">\n",
      ">Actually, I think the large-scale sample size is part of the problem.\n",
      ">It seems to me that if we were to plot all the players in baseball\n",
      ">in regard to BA vs. Clutch BA deviation we would get some kind of\n",
      ">bell curve.  (The X-axis being the +/- deviation in clutch hitting\n",
      ">vs. non-clutch;  the Y-axis being the number of players.)  Certainly\n",
      ">there would be *some* players on the extreme ends of the bell.\n",
      "\n",
      "Right.  Most definitely.\n",
      "\n",
      ">My *supposition* is that if we were to find the SAME players\n",
      ">consistently (year after year) at one end of the bell or the other,\n",
      ">then we might be able to make some reasonable conclusions about\n",
      ">*those* players (as opposed to all baseball players).\n",
      "\n",
      "This may be the root of the confusion...\n",
      "\n",
      "Please consider the follow\n",
      "==================================================\n",
      "True label: rec.sport.baseball\n",
      "Predicted label: rec.sport.baseball\n",
      "\n",
      "Content:\n",
      " From: roger@crux.Princeton.EDU (Roger Lustig)\n",
      "Subject: Re: Bonilla\n",
      "Reply-To: roger@astro.princeton.edu (Roger Lustig)\n",
      "Organization: Princeton University\n",
      "Lines: 68\n",
      "Originator: news@nimaster\n",
      "Nntp-Posting-Host: crux.princeton.edu\n",
      "\n",
      "In article <13615@news.duke.edu> fierkelab@bchm.biochem.duke.edu (Eric Roush) writes:\n",
      "\n",
      "> In article <1993Apr19.214904.29499@Princeton.EDU> roger@crux.Princeton.EDU\n",
      ">(Roger Lustig) writes:\n",
      ">>In article <steph.735253341@pegasus.cs.uiuc.edu> steph@pegasus.cs.uiuc.edu\n",
      ">(Dale Stephenson) writes:\n",
      ">>>In <1993Apr18.204643.4404@Princeton.EDU> roger@crux.Princeton.EDU (Roger\n",
      ">Lustig) writes:\n",
      "\n",
      ">>>If black players can't survive being mediocre or worse, how can McRae\n",
      ">>>and Chamberlain be explained?\n",
      "\n",
      ">>Nobody's saying it's a hard and fast rule.  My point is that white \n",
      ">>players are *likely* to stick around longer if they're mediocre.\n",
      "\n",
      ">>I went through TB III and made a list of 10-year OF and 1B who were \n",
      ">>negative in both Adjusted Batting Runs and Total Player Rating.  TPR\n",
      "==================================================\n",
      "True label: talk.politics.guns\n",
      "Predicted label: talk.politics.guns\n",
      "\n",
      "Content:\n",
      " From: loki@acca.nmsu.edu (Entropic Destroyer)\n",
      "Subject: Denver Post yanks 'Assault Ads'\n",
      "Organization: New Mexico State University\n",
      "Lines: 33\n",
      "NNTP-Posting-Host: kazak.nmsu.edu\n",
      "X-Newsreader: TIN [version 1.1 PL9]\n",
      "\n",
      "\n",
      "The Denver Post (supposed voice of the supposed Rocky Mountain Empire)\n",
      "ran the following in the 'Firearms, Supplies' classified heading on \n",
      "Friday, 23 April 1993.  If you have an opinion about their new found\n",
      "wisdom, I am told that the person to speak with is one Mr. Walters,\n",
      "(303)820-1267.\n",
      "\n",
      "\tNotice\n",
      "\n",
      "\tThe Denver Post will no longer \n",
      "\tknowingly accept any advertise-\n",
      "\tment to buy or sell assault weap-\n",
      "\tons.  The Denver Post finds that \n",
      "\tthe use of assault weapons poses\n",
      "\ta threat to the health, safety, and\n",
      "\tsecurity of its readers.\n",
      "\n",
      "Let 'em know what you think...\n",
      "\n",
      "--Dan\n",
      "--\n",
      "Spooksmoke: Revolution, Assasination, Thorium, Cobalt-60, Clintin, CIA, NSA, SHC\n",
      "  DoD #202 / loki@acca.nmsu.edu / liberty or death / taylordf@ucsu.colorado.edu \n",
      "                 Send me something even YOU can't \n",
      "==================================================\n",
      "True label: comp.graphics\n",
      "Predicted label: comp.graphics\n",
      "\n",
      "Content:\n",
      " From: aron@taos.ced.berkeley.edu (Aron Bonar)\n",
      "Subject: Re: GIF to Targa\n",
      "Organization: University of California, Berkeley\n",
      "Lines: 9\n",
      "NNTP-Posting-Host: taos.ced.berkeley.edu\n",
      "\n",
      "In article <1993Apr28.143057.8335@fuw.edu.pl>, muchor@fuw.edu.pl (Krzysztof Muchorowski) writes:\n",
      "|> Hello,\n",
      "|>    Subject says it all. I need a GIF to Targa converter, so that my\n",
      "|> dta15 could make a .FLI of them.\n",
      "|>       Krzysztof\n",
      "|> \n",
      "\n",
      "DTA will make a .FLI from GIFs as well as Targas. You don't need a converter.\n",
      "Also..get the latest version of DTA from wuarchive.wustl.edu in pub/msdos_uploads.\n",
      "\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>True Label</th>\n",
       "      <th>Predicted Label</th>\n",
       "      <th>Decoded Words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>07, 243, 38, 65, accepts, access, address, adm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rec.sport.baseball</td>\n",
       "      <td>rec.sport.baseball</td>\n",
       "      <td>10, 1024, 12, 140, 14853, 32, 50, 75, 87, able...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rec.sport.baseball</td>\n",
       "      <td>rec.sport.baseball</td>\n",
       "      <td>10, 1960, 1980, 1993apr18, 1993apr19, 1b, 25, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>talk.politics.guns</td>\n",
       "      <td>talk.politics.guns</td>\n",
       "      <td>1993, 1bfdqsj53kostz6hroshsdzlvul1, 202, 23, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>aron, berkeley, bonar, california, ced, conver...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           True Label     Predicted Label  \\\n",
       "0       comp.graphics       comp.graphics   \n",
       "1  rec.sport.baseball  rec.sport.baseball   \n",
       "2  rec.sport.baseball  rec.sport.baseball   \n",
       "3  talk.politics.guns  talk.politics.guns   \n",
       "4       comp.graphics       comp.graphics   \n",
       "\n",
       "                                       Decoded Words  \n",
       "0  07, 243, 38, 65, accepts, access, address, adm...  \n",
       "1  10, 1024, 12, 140, 14853, 32, 50, 75, 87, able...  \n",
       "2  10, 1960, 1980, 1993apr18, 1993apr19, 1b, 25, ...  \n",
       "3  1993, 1bfdqsj53kostz6hroshsdzlvul1, 202, 23, 3...  \n",
       "4  aron, berkeley, bonar, california, ced, conver...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show example news\n",
    "import pandas as pd\n",
    "\n",
    "pd.option_context('display.max_colwidth', 500)\n",
    "\n",
    "n_sample = 5\n",
    "n_charaxcters = 1000\n",
    "\n",
    "np.random.seed(73)\n",
    "sample_idx = np.random.choice(X_test.shape[0], n_sample)\n",
    "news = [newsgroup_data_test.data[i] for i in sample_idx]\n",
    "news_labels = [class_names[y_test[i]] for i in sample_idx]\n",
    "news_preds = [class_names[test_preds[i]] for i in sample_idx]\n",
    "\n",
    "for i in range(n_sample):\n",
    "    print(f\"True label: {news_labels[i]}\")\n",
    "    print(f\"Predicted label: {news_preds[i]}\")\n",
    "    print(f\"\\nContent:\\n {news[i][:n_charaxcters]}\")\n",
    "    print('='*50)\n",
    "\n",
    "\n",
    "decoded_words = [', '.join([idx2word[j] for j in np.where(X_test[i] == 1)[1]]) for i in sample_idx]\n",
    "\n",
    "df = pd.DataFrame({'True Label': news_labels, 'Predicted Label': news_preds, 'Decoded Words': decoded_words})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test against sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.00000000e+000, 1.76704423e-012, 8.18625756e-023,\n",
       "        8.27307091e-032],\n",
       "       [9.23201264e-022, 2.40051991e-034, 1.00000000e+000,\n",
       "        1.01861610e-010],\n",
       "       [9.58339542e-010, 2.39660974e-009, 9.98979339e-001,\n",
       "        1.02065778e-003],\n",
       "       [1.95015711e-122, 4.86095747e-107, 1.25694169e-090,\n",
       "        1.00000000e+000]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "bnb_sk = BernoulliNB(alpha=1.0)\n",
    "bnb_sk.fit(np.asarray(X_train), y_train)\n",
    "\n",
    "test_probs_sk = bnb_sk.predict_proba(np.asarray(X_test)[0:4])\n",
    "test_probs_sk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 4), dtype=float64, numpy=\n",
       "array([[1.00000000e+000, 1.76745994e-012, 8.18742826e-023,\n",
       "        8.27521554e-032],\n",
       "       [9.23305532e-022, 2.40163558e-034, 1.00000000e+000,\n",
       "        1.01957024e-010],\n",
       "       [9.58291465e-010, 2.39740975e-009, 9.98962940e-001,\n",
       "        1.02132169e-003],\n",
       "       [1.94974907e-122, 4.86016663e-107, 1.25595674e-090,\n",
       "        1.00000000e+000]])>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnb = BernoulliNaiveBayes(alpha=1.0)\n",
    "bnb.fit(X_train, y_train)\n",
    "bnb.predict(np.asarray(X_test)[0:4], prob=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
